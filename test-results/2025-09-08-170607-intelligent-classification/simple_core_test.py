#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†ç±»æœåŠ¡æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
ç®€åŒ–ç‰ˆæœ¬ï¼Œæµ‹è¯•æ ¸å¿ƒç»„ä»¶è€Œä¸ä¾èµ–å¤æ‚çš„æ¨¡å—ç»“æ„
"""

import json
import sys
import time
from datetime import datetime
from pathlib import Path

# åŸºç¡€æµ‹è¯•åŠŸèƒ½
def test_basic_imports():
    """æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥"""
    try:
        import jieba
        import pandas as pd
        import numpy as np
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.ensemble import RandomForestClassifier
        print("âœ… åŸºç¡€ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
        return True, "æ‰€æœ‰åŸºç¡€ä¾èµ–åº“å¯¼å…¥æˆåŠŸ"
    except Exception as e:
        print(f"âŒ åŸºç¡€ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {e}")
        return False, str(e)

def test_jieba_functionality():
    """æµ‹è¯•ä¸­æ–‡åˆ†è¯åŠŸèƒ½"""
    try:
        import jieba
        test_text = "æ±‰æ­¦å¸æ—¶æœŸå›½åŠ›å¼ºç››ï¼Œå¤šæ¬¡å‡ºå¾åŒˆå¥´ï¼Œå»ºç«‹äº†å¼ºå¤§çš„æ±‰å¸å›½ã€‚"
        tokens = list(jieba.cut(test_text))
        
        print(f"âœ… ä¸­æ–‡åˆ†è¯æˆåŠŸ")
        print(f"  ğŸ“ åŸæ–‡æœ¬: {test_text}")
        print(f"  ğŸ”¤ åˆ†è¯ç»“æœ: {tokens[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªè¯
        print(f"  ğŸ“Š è¯æ±‡æ•°é‡: {len(tokens)}")
        
        return True, {
            "original_text": test_text,
            "tokens": tokens,
            "token_count": len(tokens)
        }
    except Exception as e:
        print(f"âŒ ä¸­æ–‡åˆ†è¯æµ‹è¯•å¤±è´¥: {e}")
        return False, str(e)

def test_tfidf_functionality():
    """æµ‹è¯•TF-IDFç‰¹å¾æå–"""
    try:
        import jieba
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np
        
        # æµ‹è¯•æ–‡æ¡£
        docs = [
            "æ±‰æ­¦å¸æ—¶æœŸå›½åŠ›å¼ºç››",
            "å”æœè¯—æ­Œè‰ºæœ¯ç¹è£", 
            "å®‹æœå•†ä¸šè´¸æ˜“å‘è¾¾",
            "æ˜æœå†›äº‹æŠ€æœ¯å…ˆè¿›"
        ]
        
        # ä½¿ç”¨jiebaåˆ†è¯
        processed_docs = []
        for doc in docs:
            tokens = list(jieba.cut(doc))
            processed_docs.append(" ".join(tokens))
        
        # TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer(max_features=100, token_pattern=r'(?u)\b\w+\b')
        tfidf_matrix = vectorizer.fit_transform(processed_docs)
        feature_names = vectorizer.get_feature_names_out()
        
        print("âœ… TF-IDFç‰¹å¾æå–æˆåŠŸ")
        print(f"  ğŸ“Š æ–‡æ¡£æ•°é‡: {len(docs)}")
        print(f"  ğŸ“ˆ ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {tfidf_matrix.shape}")
        print(f"  ğŸ”¤ ç‰¹å¾è¯æ±‡æ•°: {len(feature_names)}")
        print(f"  ğŸ“ éƒ¨åˆ†ç‰¹å¾è¯: {list(feature_names[:10])}")
        
        return True, {
            "docs_count": len(docs),
            "feature_matrix_shape": tfidf_matrix.shape,
            "vocab_size": len(feature_names),
            "sample_features": list(feature_names[:10])
        }
    except Exception as e:
        print(f"âŒ TF-IDFç‰¹å¾æå–å¤±è´¥: {e}")
        return False, str(e)

def test_ml_classifier():
    """æµ‹è¯•æœºå™¨å­¦ä¹ åˆ†ç±»å™¨"""
    try:
        import jieba
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        import numpy as np
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
        texts = [
            "æ±‰æ­¦å¸å¾æˆ˜åŒˆå¥´å»ºç«‹åŠŸå‹‹",
            "å”å¤ªå®—æ²»ç†å›½å®¶æ”¿ç»©æ˜¾è‘—", 
            "æç™½æœç”«è¯—æ­Œä¼ ä¸–ç»å…¸",
            "ç‹ç»´å­Ÿæµ©ç„¶å±±æ°´è¯—ä¼˜ç¾",
            "å®‹æœå•†äººè´¸æ˜“æµ·å¤–å‘è¾¾",
            "æ˜æœéƒ‘å’Œä¸‹è¥¿æ´‹è´¸æ˜“",
            "æ˜¥ç§‹æˆ˜å›½å†›äº‹æ€æƒ³å‘å±•",
            "å­™å­å…µæ³•å†›äº‹æˆ˜ç•¥ç»å…¸"
        ]
        
        labels = [
            "æ”¿æ²»", "æ”¿æ²»", "æ–‡å­¦", "æ–‡å­¦", 
            "ç»æµ", "ç»æµ", "å†›äº‹", "å†›äº‹"
        ]
        
        # æ–‡æœ¬é¢„å¤„ç†
        processed_texts = []
        for text in texts:
            tokens = list(jieba.cut(text))
            processed_texts.append(" ".join(tokens))
        
        # ç‰¹å¾æå–
        vectorizer = TfidfVectorizer(max_features=50, token_pattern=r'(?u)\b\w+\b')
        X = vectorizer.fit_transform(processed_texts).toarray()
        y = np.array(labels)
        
        # è®­ç»ƒåˆ†ç±»å™¨
        classifier = RandomForestClassifier(n_estimators=10, random_state=42)
        classifier.fit(X, y)
        
        # é¢„æµ‹æµ‹è¯•
        test_text = "è¯—äººåˆ›ä½œä¼˜ç§€ä½œå“"
        test_tokens = list(jieba.cut(test_text))
        test_processed = " ".join(test_tokens)
        test_features = vectorizer.transform([test_processed])
        prediction = classifier.predict(test_features)[0]
        probabilities = classifier.predict_proba(test_features)[0]
        
        print("âœ… æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æµ‹è¯•æˆåŠŸ")
        print(f"  ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {len(texts)}")
        print(f"  ğŸ¯ åˆ†ç±»ç±»åˆ«: {list(set(labels))}")
        print(f"  ğŸ”® æµ‹è¯•æ–‡æœ¬: {test_text}")
        print(f"  ğŸ“ˆ é¢„æµ‹ç»“æœ: {prediction}")
        print(f"  ğŸ² é¢„æµ‹æ¦‚ç‡: {dict(zip(classifier.classes_, probabilities))}")
        
        return True, {
            "training_samples": len(texts),
            "classes": list(set(labels)),
            "test_text": test_text,
            "prediction": prediction,
            "probabilities": dict(zip(classifier.classes_, probabilities))
        }
    except Exception as e:
        print(f"âŒ æœºå™¨å­¦ä¹ åˆ†ç±»å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False, str(e)

def test_pydantic_models():
    """æµ‹è¯•Pydanticæ•°æ®æ¨¡å‹"""
    try:
        from pydantic import BaseModel, Field
        from typing import Optional, List
        from datetime import datetime
        
        # å®šä¹‰æµ‹è¯•æ¨¡å‹
        class TestClassificationRequest(BaseModel):
            project_id: str = Field(..., description="é¡¹ç›®ID")
            text_content: str = Field(..., min_length=1, description="æ–‡æœ¬å†…å®¹")
            return_probabilities: bool = Field(True, description="è¿”å›æ¦‚ç‡")
        
        class TestResponse(BaseModel):
            success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
            prediction: str = Field(..., description="é¢„æµ‹ç»“æœ")
            probabilities: Optional[dict] = Field(None, description="æ¦‚ç‡åˆ†å¸ƒ")
            timestamp: datetime = Field(default_factory=datetime.now)
        
        # æµ‹è¯•æ¨¡å‹åˆ›å»º
        request = TestClassificationRequest(
            project_id="test-project-001",
            text_content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬å†…å®¹",
            return_probabilities=True
        )
        
        response = TestResponse(
            success=True,
            prediction="æµ‹è¯•åˆ†ç±»",
            probabilities={"æµ‹è¯•åˆ†ç±»": 0.85, "å…¶ä»–": 0.15}
        )
        
        print("âœ… Pydanticæ•°æ®æ¨¡å‹æµ‹è¯•æˆåŠŸ")
        print(f"  ğŸ“ è¯·æ±‚æ¨¡å‹: {request.model_dump()}")
        print(f"  ğŸ“¤ å“åº”æ¨¡å‹: {response.model_dump()}")
        
        return True, {
            "request_model": request.model_dump(),
            "response_model": response.model_dump()
        }
    except Exception as e:
        print(f"âŒ Pydanticæ•°æ®æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False, str(e)

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ™ºèƒ½åˆ†ç±»æœåŠ¡æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print("ğŸ“ è¯´æ˜: æµ‹è¯•æ ¸å¿ƒMLå’ŒNLPåŠŸèƒ½æ¨¡å—")
    print()
    
    test_results = {
        "start_time": datetime.now().isoformat(),
        "service": "intelligent-classification-service",
        "test_type": "core_functionality",
        "tests": [],
        "summary": {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "errors": []
        }
    }
    
    tests = [
        ("åŸºç¡€ä¾èµ–å¯¼å…¥", test_basic_imports),
        ("ä¸­æ–‡åˆ†è¯åŠŸèƒ½", test_jieba_functionality),
        ("TF-IDFç‰¹å¾æå–", test_tfidf_functionality), 
        ("æœºå™¨å­¦ä¹ åˆ†ç±»å™¨", test_ml_classifier),
        ("Pydanticæ•°æ®æ¨¡å‹", test_pydantic_models)
    ]
    
    for test_name, test_func in tests:
        print(f"ğŸ”§ æµ‹è¯•{test_name}...")
        start_time = time.time()
        
        try:
            success, details = test_func()
            duration = time.time() - start_time
            
            test_results["tests"].append({
                "name": test_name,
                "status": "PASSED" if success else "FAILED",
                "duration": duration,
                "details": details,
                "error": None if success else details
            })
            
            if success:
                test_results["summary"]["passed"] += 1
            else:
                test_results["summary"]["failed"] += 1
                test_results["summary"]["errors"].append(details)
                
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"{test_name}å¼‚å¸¸: {str(e)}"
            print(f"âŒ {error_msg}")
            
            test_results["tests"].append({
                "name": test_name,
                "status": "ERROR",
                "duration": duration,
                "details": {},
                "error": error_msg
            })
            
            test_results["summary"]["failed"] += 1
            test_results["summary"]["errors"].append(error_msg)
        
        test_results["summary"]["total"] += 1
        print()
    
    # å®Œæˆæµ‹è¯•
    test_results["end_time"] = datetime.now().isoformat()
    test_results["total_duration"] = sum(test.get("duration", 0) for test in test_results["tests"])
    test_results["summary"]["success_rate"] = (
        test_results["summary"]["passed"] / test_results["summary"]["total"] * 100
        if test_results["summary"]["total"] > 0 else 0
    )
    
    # æ‰“å°æ‘˜è¦
    print("=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ¯ æ€»æµ‹è¯•æ•°: {test_results['summary']['total']}")
    print(f"âœ… é€šè¿‡: {test_results['summary']['passed']}")
    print(f"âŒ å¤±è´¥: {test_results['summary']['failed']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {test_results['summary']['success_rate']:.1f}%")
    print(f"â±ï¸  æ€»è€—æ—¶: {test_results['total_duration']:.2f}ç§’")
    
    if test_results['summary']['errors']:
        print(f"\nâ— é”™è¯¯åˆ—è¡¨:")
        for i, error in enumerate(test_results['summary']['errors'], 1):
            print(f"  {i}. {error}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    result_file = "core_functionality_test_results.json"
    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(test_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print("ğŸ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å®Œæˆ")
    
    return test_results

if __name__ == "__main__":
    results = main()