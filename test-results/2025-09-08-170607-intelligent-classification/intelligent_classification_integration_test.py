#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†ç±»æœåŠ¡é›†æˆæµ‹è¯•è„šæœ¬
æµ‹è¯•æ™ºèƒ½åˆ†ç±»æœåŠ¡çš„å®Œæ•´åŠŸèƒ½æµç¨‹
åŒ…æ‹¬é¡¹ç›®ç®¡ç†ã€è®­ç»ƒæ•°æ®ã€æ¨¡å‹è®­ç»ƒã€æ–‡æ¡£åˆ†ç±»ç­‰
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime
from typing import Dict, List, Any
import logging
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('integration_test.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntelligentClassificationIntegrationTest:
    """æ™ºèƒ½åˆ†ç±»æœåŠ¡é›†æˆæµ‹è¯•ç±»"""
    
    def __init__(self, base_url: str = "http://localhost:8007"):
        self.base_url = base_url
        self.api_prefix = "/api/v1"
        self.session = None
        self.test_results = {
            "start_time": datetime.now().isoformat(),
            "service": "intelligent-classification-service",
            "base_url": base_url,
            "tests": [],
            "summary": {
                "total": 0,
                "passed": 0,
                "failed": 0,
                "errors": []
            }
        }
        
        # æµ‹è¯•æ•°æ®
        self.test_project_id = None
        self.test_model_id = None
        self.test_training_data_ids = []
        
    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=60)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={"Content-Type": "application/json"}
        )
        
    async def teardown(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        if self.session:
            await self.session.close()
            
        # æ¸…ç†æµ‹è¯•æ•°æ®
        await self.cleanup_test_data()
    
    async def cleanup_test_data(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        try:
            # åˆ é™¤æµ‹è¯•é¡¹ç›®ï¼ˆä¼šçº§è”åˆ é™¤ç›¸å…³æ•°æ®ï¼‰
            if self.test_project_id:
                await self.make_request("DELETE", f"/projects/{self.test_project_id}")
        except Exception as e:
            logger.warning(f"æ¸…ç†æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
    
    async def make_request(self, method: str, endpoint: str, data: Dict = None, params: Dict = None) -> Dict:
        """å‘é€HTTPè¯·æ±‚"""
        url = f"{self.base_url}{self.api_prefix}{endpoint}"
        
        kwargs = {}
        if data:
            kwargs["json"] = data
        if params:
            kwargs["params"] = params
            
        async with self.session.request(method, url, **kwargs) as response:
            response_data = await response.json()
            
            if response.status >= 400:
                raise aiohttp.ClientResponseError(
                    request_info=response.request_info,
                    history=response.history,
                    status=response.status,
                    message=response_data.get("message", "è¯·æ±‚å¤±è´¥")
                )
            
            return response_data
    
    async def test_service_health(self) -> bool:
        """æµ‹è¯•æœåŠ¡å¥åº·æ£€æŸ¥"""
        test_name = "æœåŠ¡å¥åº·æ£€æŸ¥"
        logger.info(f"ğŸ¥ å¼€å§‹æµ‹è¯•: {test_name}")
        start_time = time.time()
        
        try:
            # æµ‹è¯•æ ¹è·¯å¾„
            async with self.session.get(f"{self.base_url}/") as response:
                root_data = await response.json()
                assert response.status == 200
                assert root_data["service"] == "intelligent-classification-service"
            
            # æµ‹è¯•å¥åº·æ£€æŸ¥
            async with self.session.get(f"{self.base_url}/health") as response:
                health_data = await response.json()
                assert response.status == 200
                assert health_data["service"] == "intelligent-classification-service"
                assert health_data["status"] in ["healthy", "unhealthy"]
            
            # æµ‹è¯•å°±ç»ªæ£€æŸ¥
            async with self.session.get(f"{self.base_url}/ready") as response:
                ready_data = await response.json()
                # å°±ç»ªæ£€æŸ¥å¯èƒ½å¤±è´¥ï¼ˆå¦‚æœstorage-serviceæœªå¯åŠ¨ï¼‰
                
            # æµ‹è¯•æœåŠ¡ä¿¡æ¯
            async with self.session.get(f"{self.base_url}/info") as response:
                info_data = await response.json()
                assert response.status == 200
                assert "service" in info_data
                assert "features" in info_data
                
            duration = time.time() - start_time
            self.test_results["tests"].append({
                "name": test_name,
                "status": "PASSED",
                "duration": duration,
                "details": {
                    "root_response": root_data,
                    "health_response": health_data,
                    "info_response": info_data
                }
            })
            self.test_results["summary"]["passed"] += 1
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}"
            self.test_results["tests"].append({
                "name": test_name,
                "status": "FAILED",
                "duration": duration,
                "error": error_msg,
                "traceback": traceback.format_exc()
            })
            self.test_results["summary"]["failed"] += 1
            self.test_results["summary"]["errors"].append(error_msg)
            logger.error(f"âŒ {test_name} - å¤±è´¥: {error_msg}")
            return False
    
    async def test_project_management(self) -> bool:
        """æµ‹è¯•é¡¹ç›®ç®¡ç†åŠŸèƒ½"""
        test_name = "é¡¹ç›®ç®¡ç†API"
        logger.info(f"ğŸ“‹ å¼€å§‹æµ‹è¯•: {test_name}")
        start_time = time.time()
        
        try:
            # 1. è·å–æ”¯æŒçš„åˆ†ç±»ç±»å‹
            supported_types = await self.make_request("GET", "/projects/supported/types")
            assert supported_types["success"] == True
            assert "classification_types" in supported_types["data"]
            
            # 2. åˆ›å»ºæµ‹è¯•é¡¹ç›®
            project_data = {
                "name": "é›†æˆæµ‹è¯•é¡¹ç›®",
                "description": "ç”¨äºé›†æˆæµ‹è¯•çš„å¤ä»£æ–‡çŒ®ä¸»é¢˜åˆ†ç±»é¡¹ç›®",
                "classification_type": "topic",
                "language": "zh",
                "custom_labels": ["æ”¿æ²»", "å†›äº‹", "ç»æµ", "æ–‡åŒ–", "ç¤¾ä¼š"]
            }
            
            create_response = await self.make_request("POST", "/projects", project_data)
            assert create_response["success"] == True
            self.test_project_id = create_response["data"]["id"]
            
            # 3. è·å–é¡¹ç›®è¯¦æƒ…
            project_detail = await self.make_request("GET", f"/projects/{self.test_project_id}")
            assert project_detail["success"] == True
            assert project_detail["data"]["name"] == project_data["name"]
            assert project_detail["data"]["classification_type"] == project_data["classification_type"]
            
            # 4. æ›´æ–°é¡¹ç›®
            update_data = {
                "description": "æ›´æ–°åçš„é¡¹ç›®æè¿° - é›†æˆæµ‹è¯•"
            }
            update_response = await self.make_request("PUT", f"/projects/{self.test_project_id}", update_data)
            assert update_response["success"] == True
            
            # 5. åˆ—å‡ºé¡¹ç›®
            projects_list = await self.make_request("GET", "/projects", params={"limit": 10})
            assert projects_list["success"] == True
            assert len(projects_list["data"]["projects"]) >= 1
            
            duration = time.time() - start_time
            self.test_results["tests"].append({
                "name": test_name,
                "status": "PASSED",
                "duration": duration,
                "details": {
                    "project_id": self.test_project_id,
                    "supported_types_count": len(supported_types["data"]["classification_types"]),
                    "project_created": True,
                    "project_updated": True,
                    "projects_listed": len(projects_list["data"]["projects"])
                }
            })
            self.test_results["summary"]["passed"] += 1
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"é¡¹ç›®ç®¡ç†æµ‹è¯•å¤±è´¥: {str(e)}"
            self.test_results["tests"].append({
                "name": test_name,
                "status": "FAILED",
                "duration": duration,
                "error": error_msg,
                "traceback": traceback.format_exc()
            })
            self.test_results["summary"]["failed"] += 1
            self.test_results["summary"]["errors"].append(error_msg)
            logger.error(f"âŒ {test_name} - å¤±è´¥: {error_msg}")
            return False
    
    async def test_training_data_management(self) -> bool:
        """æµ‹è¯•è®­ç»ƒæ•°æ®ç®¡ç†åŠŸèƒ½"""
        test_name = "è®­ç»ƒæ•°æ®ç®¡ç†API"
        logger.info(f"ğŸ“Š å¼€å§‹æµ‹è¯•: {test_name}")
        start_time = time.time()
        
        if not self.test_project_id:
            logger.error("é¡¹ç›®IDä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒæ•°æ®æµ‹è¯•")
            return False
        
        try:
            # 1. æ·»åŠ å•æ¡è®­ç»ƒæ•°æ®
            training_data = {
                "project_id": self.test_project_id,
                "text_content": "æ±‰æ­¦å¸æ—¶æœŸï¼Œå›½åŠ›å¼ºç››ï¼Œå¤šæ¬¡å‡ºå¾åŒˆå¥´ï¼Œå¼€æ‹“ç–†åœŸï¼Œå»ºç«‹äº†å¼ºå¤§çš„æ±‰å¸å›½ã€‚",
                "true_label": "æ”¿æ²»",
                "label_confidence": 1.0,
                "data_source": "integration_test"
            }
            
            add_response = await self.make_request("POST", "/data/training-data", training_data)
            assert add_response["success"] == True
            training_data_id = add_response["data"]["id"]
            self.test_training_data_ids.append(training_data_id)
            
            # 2. æ‰¹é‡æ·»åŠ è®­ç»ƒæ•°æ®
            batch_data = {
                "project_id": self.test_project_id,
                "training_data": [
                    {
                        "text_content": "å”æœè¯—æ­Œç¹è£ï¼Œæç™½ã€æœç”«ç­‰è¯—äººåˆ›ä½œäº†è®¸å¤šä¼ ä¸–ä½³ä½œã€‚",
                        "true_label": "æ–‡åŒ–"
                    },
                    {
                        "text_content": "å®‹æœå•†ä¸šå‘è¾¾ï¼Œæµ·ä¸Šä¸ç»¸ä¹‹è·¯è´¸æ˜“å…´ç››ï¼Œç»æµç¹è£ã€‚",
                        "true_label": "ç»æµ"
                    },
                    {
                        "text_content": "æ˜æœå†›é˜Ÿè£…å¤‡ç²¾è‰¯ï¼Œç«å™¨ä½¿ç”¨å¹¿æ³›ï¼Œå†›äº‹å®åŠ›å¼ºå¤§ã€‚",
                        "true_label": "å†›äº‹"
                    },
                    {
                        "text_content": "æ¸…æœç¤¾ä¼šç­‰çº§æ£®ä¸¥ï¼Œæ»¡æ±‰æœ‰åˆ«ï¼Œç¤¾ä¼šçŸ›ç›¾å°–é”ã€‚",
                        "true_label": "ç¤¾ä¼š"
                    }
                ]
            }
            
            batch_response = await self.make_request("POST", "/data/training-data/batch", batch_data)
            assert batch_response["success"] == True
            assert batch_response["data"]["successful_added"] == 4
            
            # 3. è·å–è®­ç»ƒæ•°æ®
            data_list = await self.make_request("GET", f"/data/training-data/{self.test_project_id}", 
                                               params={"limit": 10})
            assert data_list["success"] == True
            assert len(data_list["data"]["data"]) >= 5
            
            # 4. è·å–è®­ç»ƒæ•°æ®ç»Ÿè®¡
            stats_response = await self.make_request("GET", f"/data/training-data/{self.test_project_id}/statistics")
            assert stats_response["success"] == True
            
            # 5. éªŒè¯è®­ç»ƒæ•°æ®è´¨é‡
            validation_response = await self.make_request("POST", f"/data/training-data/{self.test_project_id}/validate")
            assert validation_response["success"] == True
            assert "quality_score" in validation_response["data"]
            
            duration = time.time() - start_time
            self.test_results["tests"].append({
                "name": test_name,
                "status": "PASSED", 
                "duration": duration,
                "details": {
                    "single_data_added": True,
                    "batch_data_added": batch_response["data"]["successful_added"],
                    "total_data_count": len(data_list["data"]["data"]),
                    "quality_score": validation_response["data"]["quality_score"]
                }
            })
            self.test_results["summary"]["passed"] += 1
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"è®­ç»ƒæ•°æ®ç®¡ç†æµ‹è¯•å¤±è´¥: {str(e)}"
            self.test_results["tests"].append({
                "name": test_name,
                "status": "FAILED",
                "duration": duration,
                "error": error_msg,
                "traceback": traceback.format_exc()
            })
            self.test_results["summary"]["failed"] += 1
            self.test_results["summary"]["errors"].append(error_msg)
            logger.error(f"âŒ {test_name} - å¤±è´¥: {error_msg}")
            return False
    
    async def test_model_training(self) -> bool:
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒåŠŸèƒ½"""
        test_name = "æ¨¡å‹è®­ç»ƒAPI"
        logger.info(f"ğŸ¤– å¼€å§‹æµ‹è¯•: {test_name}")
        start_time = time.time()
        
        if not self.test_project_id:
            logger.error("é¡¹ç›®IDä¸ºç©ºï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒæµ‹è¯•")
            return False
        
        try:
            # 1. è·å–æ”¯æŒçš„æ¨¡å‹ç±»å‹
            model_types = await self.make_request("GET", "/models/types/supported")
            assert model_types["success"] == True
            assert "model_types" in model_types["data"]
            
            # 2. å¯åŠ¨æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨å¿«é€Ÿçš„éšæœºæ£®æ—ï¼‰
            training_request = {
                "project_id": self.test_project_id,
                "model_type": "random_forest",
                "feature_extractor": "tfidf",
                "hyperparameters": {
                    "n_estimators": 10,  # å‡å°‘æ ‘çš„æ•°é‡ä»¥åŠ å¿«è®­ç»ƒ
                    "max_depth": 5
                },
                "training_config": {
                    "test_size": 0.3,
                    "cv_folds": 3
                }
            }
            
            train_response = await self.make_request("POST", "/models/train", training_request)
            assert train_response["success"] == True
            self.test_model_id = train_response["data"]["model_id"]
            
            # 3. ç­‰å¾…è®­ç»ƒå®Œæˆï¼ˆè½®è¯¢æ£€æŸ¥ï¼‰
            max_wait_time = 120  # æœ€å¤šç­‰å¾…2åˆ†é’Ÿ
            wait_start = time.time()
            training_completed = False
            
            while time.time() - wait_start < max_wait_time:
                model_info = await self.make_request("GET", f"/models/{self.test_model_id}")
                if model_info["success"]:
                    status = model_info["data"].get("status")
                    if status == "completed":
                        training_completed = True
                        break
                    elif status == "failed":
                        raise Exception(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {model_info['data'].get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                
                await asyncio.sleep(5)  # ç­‰å¾…5ç§’åå†æ¬¡æ£€æŸ¥
            
            if not training_completed:
                logger.warning("æ¨¡å‹è®­ç»ƒè¶…æ—¶ï¼Œç»§ç»­å…¶ä»–æµ‹è¯•")
                # ä¸ç®—ä½œå¤±è´¥ï¼Œå› ä¸ºå¯èƒ½æ˜¯æ­£å¸¸çš„é•¿æ—¶é—´è®­ç»ƒ
            
            # 4. è·å–é¡¹ç›®çš„æ¨¡å‹åˆ—è¡¨
            models_list = await self.make_request("GET", f"/models/project/{self.test_project_id}")
            assert models_list["success"] == True
            assert len(models_list["data"]["models"]) >= 1
            
            # 5. å¦‚æœè®­ç»ƒå®Œæˆï¼Œæµ‹è¯•æ¿€æ´»æ¨¡å‹
            if training_completed:
                activate_response = await self.make_request("POST", f"/models/{self.test_model_id}/activate")
                assert activate_response["success"] == True
                
                # è·å–æ´»è·ƒæ¨¡å‹
                active_model = await self.make_request("GET", f"/models/project/{self.test_project_id}/active")
                assert active_model["success"] == True
                assert active_model["data"]["model_id"] == self.test_model_id
            
            duration = time.time() - start_time
            self.test_results["tests"].append({
                "name": test_name,
                "status": "PASSED",
                "duration": duration,
                "details": {
                    "model_id": self.test_model_id,
                    "training_started": True,
                    "training_completed": training_completed,
                    "models_count": len(models_list["data"]["models"]),
                    "model_activated": training_completed
                }
            })
            self.test_results["summary"]["passed"] += 1
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"æ¨¡å‹è®­ç»ƒæµ‹è¯•å¤±è´¥: {str(e)}"
            self.test_results["tests"].append({
                "name": test_name,
                "status": "FAILED",
                "duration": duration,
                "error": error_msg,
                "traceback": traceback.format_exc()
            })
            self.test_results["summary"]["failed"] += 1
            self.test_results["summary"]["errors"].append(error_msg)
            logger.error(f"âŒ {test_name} - å¤±è´¥: {error_msg}")
            return False
    
    async def test_document_classification(self) -> bool:
        """æµ‹è¯•æ–‡æ¡£åˆ†ç±»åŠŸèƒ½"""
        test_name = "æ–‡æ¡£åˆ†ç±»API"
        logger.info(f"ğŸ” å¼€å§‹æµ‹è¯•: {test_name}")
        start_time = time.time()
        
        if not self.test_project_id:
            logger.error("é¡¹ç›®IDä¸ºç©ºï¼Œè·³è¿‡æ–‡æ¡£åˆ†ç±»æµ‹è¯•")
            return False
        
        try:
            # 1. å•æ–‡æ¡£åˆ†ç±»æµ‹è¯•
            classification_request = {
                "project_id": self.test_project_id,
                "text_content": "åº·ç†™çš‡å¸åœ¨ä½æœŸé—´ï¼Œå®è¡Œä»æ”¿ï¼Œå›½æ³°æ°‘å®‰ï¼Œæ˜¯æ¸…æœçš„ç››ä¸–ã€‚",
                "return_probabilities": True,
                "return_explanation": True
            }
            
            single_result = await self.make_request("POST", "/classify/single", classification_request)
            assert single_result["success"] == True
            assert "predicted_label" in single_result["data"]
            assert "confidence_score" in single_result["data"]
            
            # 2. æ‰¹é‡æ–‡æ¡£åˆ†ç±»æµ‹è¯•
            batch_request = {
                "project_id": self.test_project_id,
                "documents": [
                    {"text_content": "ç§¦å§‹çš‡ç»Ÿä¸€å…­å›½ï¼Œå»ºç«‹ä¸­å¤®é›†æƒåˆ¶åº¦ã€‚"},
                    {"text_content": "å”ä»£è¯—æ­Œè‰ºæœ¯è¾¾åˆ°é¡¶å³°ï¼Œè¯—äººè¾ˆå‡ºã€‚"},
                    {"text_content": "å®‹æœå•†è´¸å‘è¾¾ï¼Œå¸‚æ°‘ç»æµç¹è£ã€‚"}
                ],
                "return_probabilities": True
            }
            
            batch_result = await self.make_request("POST", "/classify/batch", batch_request)
            assert batch_result["success"] == True
            assert batch_result["data"]["total_documents"] == 3
            assert len(batch_result["data"]["results"]) == 3
            
            # 3. å¸¦è¯¦ç»†è§£é‡Šçš„åˆ†ç±»
            explanation_request = {
                "project_id": self.test_project_id,
                "text_content": "æ˜æœæµ·ç¦æ”¿ç­–é™åˆ¶äº†å¯¹å¤–è´¸æ˜“çš„å‘å±•ã€‚",
                "return_probabilities": True,
                "return_explanation": True
            }
            
            explanation_result = await self.make_request("POST", "/classify/predict-with-explanation", 
                                                       explanation_request)
            assert explanation_result["success"] == True
            assert "classification_result" in explanation_result["data"]
            assert "decision_process" in explanation_result["data"]
            
            # 4. è·å–åˆ†ç±»å†å²
            history_result = await self.make_request("GET", f"/classify/history/{self.test_project_id}",
                                                   params={"limit": 10})
            assert history_result["success"] == True
            
            # 5. è·å–åˆ†ç±»ç»Ÿè®¡
            stats_result = await self.make_request("GET", f"/classify/statistics/{self.test_project_id}")
            assert stats_result["success"] == True
            
            duration = time.time() - start_time
            self.test_results["tests"].append({
                "name": test_name,
                "status": "PASSED",
                "duration": duration,
                "details": {
                    "single_classification": {
                        "predicted_label": single_result["data"]["predicted_label"],
                        "confidence": single_result["data"]["confidence_score"]
                    },
                    "batch_classification": {
                        "total_documents": batch_result["data"]["total_documents"],
                        "successful": batch_result["data"]["successful_classifications"]
                    },
                    "explanation_provided": "decision_process" in explanation_result["data"],
                    "history_retrieved": True,
                    "statistics_retrieved": True
                }
            })
            self.test_results["summary"]["passed"] += 1
            logger.info(f"âœ… {test_name} - é€šè¿‡ ({duration:.2f}s)")
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            error_msg = f"æ–‡æ¡£åˆ†ç±»æµ‹è¯•å¤±è´¥: {str(e)}"
            self.test_results["tests"].append({
                "name": test_name,
                "status": "FAILED",
                "duration": duration,
                "error": error_msg,
                "traceback": traceback.format_exc()
            })
            self.test_results["summary"]["failed"] += 1
            self.test_results["summary"]["errors"].append(error_msg)
            logger.error(f"âŒ {test_name} - å¤±è´¥: {error_msg}")
            return False
    
    async def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†ç±»æœåŠ¡é›†æˆæµ‹è¯•...")
        
        await self.setup()
        
        try:
            # æ›´æ–°æµ‹è¯•æ€»æ•°
            self.test_results["summary"]["total"] = 5
            
            # ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
            await self.test_service_health()
            await self.test_project_management()
            await self.test_training_data_management()
            await self.test_model_training()
            await self.test_document_classification()
            
        except Exception as e:
            logger.error(f"é›†æˆæµ‹è¯•å¼‚å¸¸: {e}")
            self.test_results["summary"]["errors"].append(f"æ•´ä½“æµ‹è¯•å¼‚å¸¸: {str(e)}")
        
        finally:
            await self.teardown()
            
        # å®Œæˆæµ‹è¯•ç»“æœ
        self.test_results["end_time"] = datetime.now().isoformat()
        self.test_results["total_duration"] = sum(
            test.get("duration", 0) for test in self.test_results["tests"]
        )
        
        # è®¡ç®—æˆåŠŸç‡
        passed = self.test_results["summary"]["passed"]
        total = self.test_results["summary"]["total"]
        self.test_results["summary"]["success_rate"] = (passed / total * 100) if total > 0 else 0
        
        return self.test_results

async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª æ™ºèƒ½åˆ†ç±»æœåŠ¡é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    tester = IntelligentClassificationIntegrationTest()
    
    # è¿è¡Œæµ‹è¯•
    results = await tester.run_all_tests()
    
    # æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ¯ æ€»æµ‹è¯•æ•°: {results['summary']['total']}")
    print(f"âœ… é€šè¿‡: {results['summary']['passed']}")
    print(f"âŒ å¤±è´¥: {results['summary']['failed']}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
    print(f"â±ï¸  æ€»è€—æ—¶: {results['total_duration']:.2f}ç§’")
    
    if results['summary']['errors']:
        print(f"\nâ— é”™è¯¯åˆ—è¡¨:")
        for i, error in enumerate(results['summary']['errors'], 1):
            print(f"  {i}. {error}")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open("integration_test_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: integration_test_results.json")
    print("ğŸ é›†æˆæµ‹è¯•å®Œæˆ")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())