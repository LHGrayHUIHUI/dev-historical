"""
NLPæœåŠ¡æ•°æ®æ¨¡å‹å•å…ƒæµ‹è¯•

æµ‹è¯•NLPæœåŠ¡çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬ï¼š
- è¯·æ±‚å’Œå“åº”æ¨¡å‹éªŒè¯
- æšä¸¾ç±»å‹å®šä¹‰
- æ•°æ®éªŒè¯è§„åˆ™
- è¾¹ç•Œæ¡ä»¶å¤„ç†

ä½œè€…: Quinn (æµ‹è¯•æ¶æ„å¸ˆ)
åˆ›å»ºæ—¶é—´: 2025-09-09
ç‰ˆæœ¬: 1.0.0
"""

import pytest
from datetime import datetime
from uuid import uuid4
from pydantic import ValidationError

from src.schemas.nlp_schemas import (
    ProcessingType, ProcessingStatus, Language, NLPEngine, SentimentLabel,
    BaseResponse, ErrorResponse, TextProcessRequest, BatchProcessRequest
)


class TestEnumTypes:
    """æšä¸¾ç±»å‹æµ‹è¯•"""
    
    def test_processing_type_enum(self):
        """æµ‹è¯•å¤„ç†ç±»å‹æšä¸¾"""
        assert ProcessingType.SEGMENTATION == "segmentation"
        assert ProcessingType.POS_TAGGING == "pos_tagging"
        assert ProcessingType.NER == "ner"
        assert ProcessingType.SENTIMENT == "sentiment"
        assert ProcessingType.KEYWORDS == "keywords"
        assert ProcessingType.SUMMARY == "summary"
        assert ProcessingType.SIMILARITY == "similarity"
        assert ProcessingType.BATCH == "batch"
        
        # æµ‹è¯•æšä¸¾è½¬æ¢
        assert ProcessingType("segmentation") == ProcessingType.SEGMENTATION
        
        # æµ‹è¯•æ— æ•ˆå€¼
        with pytest.raises(ValueError):
            ProcessingType("invalid_type")
    
    def test_processing_status_enum(self):
        """æµ‹è¯•å¤„ç†çŠ¶æ€æšä¸¾"""
        assert ProcessingStatus.PENDING == "pending"
        assert ProcessingStatus.PROCESSING == "processing" 
        assert ProcessingStatus.COMPLETED == "completed"
        assert ProcessingStatus.FAILED == "failed"
    
    def test_language_enum(self):
        """æµ‹è¯•è¯­è¨€æšä¸¾"""
        assert Language.CHINESE == "zh"
        assert Language.ENGLISH == "en"
        assert Language.CLASSICAL_CHINESE == "zh-classical"
    
    def test_nlp_engine_enum(self):
        """æµ‹è¯•NLPå¼•æ“æšä¸¾"""
        assert NLPEngine.SPACY == "spacy"
        assert NLPEngine.JIEBA == "jieba"
        assert NLPEngine.HANLP == "hanlp"
        assert NLPEngine.TRANSFORMERS == "transformers"
    
    def test_sentiment_label_enum(self):
        """æµ‹è¯•æƒ…æ„Ÿæ ‡ç­¾æšä¸¾"""
        assert SentimentLabel.POSITIVE == "positive"
        assert SentimentLabel.NEGATIVE == "negative"
        assert SentimentLabel.NEUTRAL == "neutral"


class TestBaseModels:
    """åŸºç¡€æ¨¡å‹æµ‹è¯•"""
    
    def test_base_response_creation(self):
        """æµ‹è¯•åŸºç¡€å“åº”åˆ›å»º"""
        response = BaseResponse()
        
        assert response.success is True
        assert response.message == ""
        assert isinstance(response.timestamp, datetime)
        
        # æµ‹è¯•è‡ªå®šä¹‰å€¼
        custom_response = BaseResponse(
            success=False,
            message="æµ‹è¯•æ¶ˆæ¯"
        )
        
        assert custom_response.success is False
        assert custom_response.message == "æµ‹è¯•æ¶ˆæ¯"
    
    def test_error_response_creation(self):
        """æµ‹è¯•é”™è¯¯å“åº”åˆ›å»º"""
        error_response = ErrorResponse(
            message="æµ‹è¯•é”™è¯¯",
            error_code="TEST_ERROR",
            error_details={"field": "value"}
        )
        
        assert error_response.success is False
        assert error_response.message == "æµ‹è¯•é”™è¯¯"
        assert error_response.error_code == "TEST_ERROR"
        assert error_response.error_details == {"field": "value"}
    
    def test_base_response_json_serialization(self):
        """æµ‹è¯•å“åº”JSONåºåˆ—åŒ–"""
        response = BaseResponse(message="æµ‹è¯•")
        json_data = response.dict()
        
        assert "success" in json_data
        assert "message" in json_data
        assert "timestamp" in json_data
        assert json_data["success"] is True
        assert json_data["message"] == "æµ‹è¯•"


class TestRequestModels:
    """è¯·æ±‚æ¨¡å‹æµ‹è¯•"""
    
    def test_text_process_request_valid(self):
        """æµ‹è¯•æœ‰æ•ˆçš„æ–‡æœ¬å¤„ç†è¯·æ±‚"""
        request = TextProcessRequest(
            text="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            processing_type=ProcessingType.SEGMENTATION,
            language=Language.CHINESE
        )
        
        assert request.text == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        assert request.processing_type == ProcessingType.SEGMENTATION
        assert request.language == Language.CHINESE
        assert request.engine is None
        assert request.config == {}
        assert request.async_mode is False
        assert request.dataset_id is None
    
    def test_text_process_request_with_all_fields(self):
        """æµ‹è¯•åŒ…å«æ‰€æœ‰å­—æ®µçš„æ–‡æœ¬å¤„ç†è¯·æ±‚"""
        request = TextProcessRequest(
            text="å®Œæ•´å‚æ•°æµ‹è¯•æ–‡æœ¬",
            processing_type=ProcessingType.POS_TAGGING,
            language=Language.ENGLISH,
            engine=NLPEngine.SPACY,
            config={"threshold": 0.8},
            async_mode=True,
            dataset_id="test-dataset-123"
        )
        
        assert request.text == "å®Œæ•´å‚æ•°æµ‹è¯•æ–‡æœ¬"
        assert request.processing_type == ProcessingType.POS_TAGGING
        assert request.language == Language.ENGLISH
        assert request.engine == NLPEngine.SPACY
        assert request.config == {"threshold": 0.8}
        assert request.async_mode is True
        assert request.dataset_id == "test-dataset-123"
    
    def test_text_process_request_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬è¯·æ±‚éªŒè¯"""
        with pytest.raises(ValidationError) as exc_info:
            TextProcessRequest(
                text="   ",  # åªæœ‰ç©ºç™½å­—ç¬¦
                processing_type=ProcessingType.SEGMENTATION
            )
        
        assert "æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º" in str(exc_info.value)
    
    def test_text_process_request_missing_required_fields(self):
        """æµ‹è¯•ç¼ºå°‘å¿…éœ€å­—æ®µ"""
        with pytest.raises(ValidationError):
            TextProcessRequest(processing_type=ProcessingType.SEGMENTATION)
        
        with pytest.raises(ValidationError):
            TextProcessRequest(text="æµ‹è¯•æ–‡æœ¬")
    
    def test_text_process_request_too_long_text(self):
        """æµ‹è¯•æ–‡æœ¬è¿‡é•¿éªŒè¯"""
        long_text = "a" * 1000001  # è¶…è¿‡æœ€å¤§é•¿åº¦
        
        with pytest.raises(ValidationError) as exc_info:
            TextProcessRequest(
                text=long_text,
                processing_type=ProcessingType.SEGMENTATION
            )
        
        assert "String should have at most 1000000 characters" in str(exc_info.value)
    
    def test_batch_process_request_valid(self):
        """æµ‹è¯•æœ‰æ•ˆçš„æ‰¹é‡å¤„ç†è¯·æ±‚"""
        request = BatchProcessRequest(
            texts=["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"],
            processing_type=ProcessingType.SENTIMENT
        )
        
        assert len(request.texts) == 3
        assert request.texts[0] == "æ–‡æœ¬1"
        assert request.processing_type == ProcessingType.SENTIMENT
        assert request.language == Language.CHINESE
    
    def test_batch_process_request_too_many_texts(self):
        """æµ‹è¯•æ‰¹é‡è¯·æ±‚æ–‡æœ¬æ•°é‡é™åˆ¶"""
        too_many_texts = [f"æ–‡æœ¬{i}" for i in range(51)]  # è¶…è¿‡æœ€å¤§æ•°é‡50
        
        with pytest.raises(ValidationError) as exc_info:
            BatchProcessRequest(
                texts=too_many_texts,
                processing_type=ProcessingType.SEGMENTATION
            )
        
        assert "List should have at most 50 items" in str(exc_info.value)
    
    def test_batch_process_request_empty_list(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬åˆ—è¡¨"""
        with pytest.raises(ValidationError):
            BatchProcessRequest(
                texts=[],
                processing_type=ProcessingType.SEGMENTATION
            )


class TestModelValidation:
    """æ¨¡å‹éªŒè¯æµ‹è¯•"""
    
    def test_enum_string_conversion(self):
        """æµ‹è¯•æšä¸¾å­—ç¬¦ä¸²è½¬æ¢"""
        # æµ‹è¯•å­—ç¬¦ä¸²ç›´æ¥ä¼ å…¥
        request = TextProcessRequest(
            text="æµ‹è¯•æ–‡æœ¬",
            processing_type="segmentation",  # å­—ç¬¦ä¸²
            language="zh",  # å­—ç¬¦ä¸²
            engine="jieba"  # å­—ç¬¦ä¸²
        )
        
        assert request.processing_type == ProcessingType.SEGMENTATION
        assert request.language == Language.CHINESE
        assert request.engine == NLPEngine.JIEBA
    
    def test_invalid_enum_values(self):
        """æµ‹è¯•æ— æ•ˆæšä¸¾å€¼"""
        with pytest.raises(ValidationError):
            TextProcessRequest(
                text="æµ‹è¯•æ–‡æœ¬",
                processing_type="invalid_type"
            )
        
        with pytest.raises(ValidationError):
            TextProcessRequest(
                text="æµ‹è¯•æ–‡æœ¬",
                processing_type=ProcessingType.SEGMENTATION,
                language="invalid_language"
            )
    
    def test_config_dict_validation(self):
        """æµ‹è¯•é…ç½®å­—å…¸éªŒè¯"""
        # æœ‰æ•ˆé…ç½®
        request = TextProcessRequest(
            text="æµ‹è¯•æ–‡æœ¬",
            processing_type=ProcessingType.SEGMENTATION,
            config={
                "threshold": 0.8,
                "max_length": 1000,
                "enable_pos": True,
                "custom_dict": ["word1", "word2"]
            }
        )
        
        assert isinstance(request.config, dict)
        assert request.config["threshold"] == 0.8
        assert request.config["enable_pos"] is True
    
    def test_optional_fields_default_values(self):
        """æµ‹è¯•å¯é€‰å­—æ®µé»˜è®¤å€¼"""
        request = TextProcessRequest(
            text="æœ€å°å‚æ•°æµ‹è¯•",
            processing_type=ProcessingType.KEYWORDS
        )
        
        # æ£€æŸ¥é»˜è®¤å€¼
        assert request.language == Language.CHINESE
        assert request.engine is None
        assert request.config == {}
        assert request.async_mode is False
        assert request.dataset_id is None


class TestModelSerialization:
    """æ¨¡å‹åºåˆ—åŒ–æµ‹è¯•"""
    
    def test_request_model_dict_export(self):
        """æµ‹è¯•è¯·æ±‚æ¨¡å‹å­—å…¸å¯¼å‡º"""
        request = TextProcessRequest(
            text="åºåˆ—åŒ–æµ‹è¯•",
            processing_type=ProcessingType.NER,
            language=Language.CLASSICAL_CHINESE,
            config={"model": "bert-base"}
        )
        
        data = request.dict()
        
        assert data["text"] == "åºåˆ—åŒ–æµ‹è¯•"
        assert data["processing_type"] == "ner"
        assert data["language"] == "zh-classical"
        assert data["config"]["model"] == "bert-base"
    
    def test_response_model_json_export(self):
        """æµ‹è¯•å“åº”æ¨¡å‹JSONå¯¼å‡º"""
        response = BaseResponse(
            success=True,
            message="å¤„ç†å®Œæˆ"
        )
        
        json_str = response.json()
        assert "success" in json_str
        assert "message" in json_str
        assert "timestamp" in json_str
        assert "true" in json_str.lower()
        assert "å¤„ç†å®Œæˆ" in json_str
    
    def test_model_from_dict(self):
        """æµ‹è¯•ä»å­—å…¸åˆ›å»ºæ¨¡å‹"""
        data = {
            "text": "å­—å…¸åˆ›å»ºæµ‹è¯•",
            "processing_type": "similarity",
            "language": "en",
            "engine": "transformers",
            "async_mode": True
        }
        
        request = TextProcessRequest(**data)
        
        assert request.text == "å­—å…¸åˆ›å»ºæµ‹è¯•"
        assert request.processing_type == ProcessingType.SIMILARITY
        assert request.language == Language.ENGLISH
        assert request.engine == NLPEngine.TRANSFORMERS
        assert request.async_mode is True


class TestEdgeCases:
    """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
    
    def test_unicode_text_handling(self):
        """æµ‹è¯•Unicodeæ–‡æœ¬å¤„ç†"""
        unicode_texts = [
            "ä¸­æ–‡æµ‹è¯•",
            "English test",
            "Ã‰moji test ğŸ˜€",
            "æ··åˆlanguageæµ‹è¯•",
            "å¤ä»£æ¼¢å­—æ¸¬è©¦",
            "ç‰¹æ®Šç¬¦å·Â©Â®â„¢â„ "
        ]
        
        for text in unicode_texts:
            request = TextProcessRequest(
                text=text,
                processing_type=ProcessingType.SEGMENTATION
            )
            assert request.text == text
    
    def test_whitespace_handling(self):
        """æµ‹è¯•ç©ºç™½å­—ç¬¦å¤„ç†"""
        # æµ‹è¯•å¼€å¤´ç»“å°¾ç©ºç™½å­—ç¬¦ï¼ˆåº”è¯¥è¢«ä¿ç•™ï¼Œä½†ä¸èƒ½å…¨ä¸ºç©ºç™½ï¼‰
        request = TextProcessRequest(
            text="  æ­£å¸¸æ–‡æœ¬  ",
            processing_type=ProcessingType.SEGMENTATION
        )
        assert request.text == "  æ­£å¸¸æ–‡æœ¬  "
        
        # æµ‹è¯•å„ç§ç©ºç™½å­—ç¬¦
        whitespace_text = "æ–‡æœ¬\tåŒ…å«\nå„ç§\rç©ºç™½\u00A0å­—ç¬¦"
        request = TextProcessRequest(
            text=whitespace_text,
            processing_type=ProcessingType.SEGMENTATION
        )
        assert request.text == whitespace_text
    
    def test_boundary_length_texts(self):
        """æµ‹è¯•è¾¹ç•Œé•¿åº¦æ–‡æœ¬"""
        # æµ‹è¯•æœ€å¤§å…è®¸é•¿åº¦çš„æ–‡æœ¬
        max_length_text = "a" * 1000000
        request = TextProcessRequest(
            text=max_length_text,
            processing_type=ProcessingType.SEGMENTATION
        )
        assert len(request.text) == 1000000
        
        # æµ‹è¯•å•å­—ç¬¦æ–‡æœ¬
        single_char_request = TextProcessRequest(
            text="å­—",
            processing_type=ProcessingType.SEGMENTATION
        )
        assert single_char_request.text == "å­—"
    
    def test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_texts = [
            "åŒ…å«æ ‡ç‚¹ï¼šï¼ï¼Ÿã€‚ï¼Œï¼›ï¼š",
            "æ‹¬å·æµ‹è¯•ï¼ˆï¼‰ã€ã€‘ã€Œã€",
            "å¼•å·æµ‹è¯•""''",
            "æ•°å­—æµ‹è¯•123456789",
            "ç¬¦å·æµ‹è¯•@#$%^&*",
            "æ§åˆ¶å­—ç¬¦æµ‹è¯•\x00\x01\x02"  # åŒ…å«æ§åˆ¶å­—ç¬¦
        ]
        
        for text in special_texts:
            # å¤§éƒ¨åˆ†ç‰¹æ®Šå­—ç¬¦åº”è¯¥è¢«æ¥å—
            try:
                request = TextProcessRequest(
                    text=text,
                    processing_type=ProcessingType.SEGMENTATION
                )
                assert request.text == text
            except ValidationError:
                # æŸäº›æ§åˆ¶å­—ç¬¦å¯èƒ½ä¼šè¢«æ‹’ç»ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                pass