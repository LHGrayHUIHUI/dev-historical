"""
AIæ¨¡å‹æœåŠ¡ä¸»åº”ç”¨
"""

import logging
import asyncio
from contextlib import asynccontextmanager
from typing import Dict, Any

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from .config.settings import get_settings
from .controllers.chat_controller import create_chat_controller
from .controllers.models_controller import create_models_controller
from .controllers.status_controller import create_status_controller
from .controllers.models_management_controller import create_models_management_controller
from .services.ai_service_simplified import get_ai_service


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/ai-model-service.log')
    ]
)

logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶çš„åˆå§‹åŒ–
    logger.info("Starting AI Model Service...")
    
    try:
        # åˆå§‹åŒ–AIæœåŠ¡
        ai_service = await get_ai_service()
        logger.info("AI Model Service initialized successfully")
        
        yield
        
    except Exception as e:
        logger.error(f"Failed to initialize AI Model Service: {e}")
        raise
    
    finally:
        # å…³é—­æ—¶çš„æ¸…ç†
        logger.info("Shutting down AI Model Service...")
        try:
            ai_service = await get_ai_service()
            await ai_service.shutdown()
            logger.info("AI Model Service shutdown completed")
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")


# åˆ›å»ºFastAPIåº”ç”¨ - Create FastAPI Application
app = FastAPI(
    title="AIæ¨¡å‹ç»Ÿä¸€æœåŠ¡ - Unified AI Model Service",
    description="""
## ğŸ¤– AIæ¨¡å‹ç»Ÿä¸€è°ƒç”¨å’Œç®¡ç†æœåŠ¡

### æ ¸å¿ƒåŠŸèƒ½ Core Features
- **å¤šAIæä¾›å•†æ”¯æŒ** - æ”¯æŒGoogle Geminiã€OpenAI GPTã€Anthropic Claudeç­‰ä¸»æµAIæ¨¡å‹
- **æ™ºèƒ½æ¨¡å‹è·¯ç”±** - è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¯ç”¨æ¨¡å‹ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
- **ç»Ÿä¸€APIæ¥å£** - å…¼å®¹OpenAI ChatGPT APIæ ¼å¼ï¼Œç®€åŒ–é›†æˆå’Œè¿ç§»
- **åŠ¨æ€æ¨¡å‹ç®¡ç†** - è¿è¡Œæ—¶æ·»åŠ ã€æ›´æ–°ã€åˆ é™¤AIæ¨¡å‹é…ç½®
- **æµå¼å“åº”æ”¯æŒ** - å®æ—¶å¯¹è¯ä½“éªŒï¼Œæ”¯æŒServer-Sent Events
- **è¿æ¥æµ‹è¯•éªŒè¯** - è‡ªåŠ¨éªŒè¯æ¨¡å‹å¯ç”¨æ€§å’Œæ€§èƒ½ç›‘æ§

### æ”¯æŒçš„AIæä¾›å•† Supported Providers
- ğŸ”¥ **Google Gemini** - gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash
- ğŸš€ **OpenAI GPT** - gpt-3.5-turbo, gpt-4, gpt-4-turbo, gpt-4o
- ğŸ’ **Anthropic Claude** - claude-3-haiku, claude-3-sonnet, claude-3-opus
- ğŸ  **æœ¬åœ°æ¨¡å‹** - æ”¯æŒOllamaã€vLLMã€FastChatç­‰æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ

### APIç«¯ç‚¹æ¦‚è§ˆ API Endpoints
- **èŠå¤©å®Œæˆ** `POST /api/v1/chat/completions` - æ ‡å‡†AIå¯¹è¯æ¥å£
- **æµå¼èŠå¤©** `POST /api/v1/chat/completions/stream` - å®æ—¶å¯¹è¯æµ
- **æ¨¡å‹åˆ—è¡¨** `GET /api/v1/models/` - è·å–å¯ç”¨æ¨¡å‹
- **æ¨¡å‹ç®¡ç†** `/api/v1/models/management/` - CRUDæ¨¡å‹é…ç½®
- **å¥åº·æ£€æŸ¥** `GET /health` - æœåŠ¡çŠ¶æ€ç›‘æ§
- **ç³»ç»Ÿä¿¡æ¯** `GET /info` - è¯¦ç»†æœåŠ¡ä¿¡æ¯

### æŠ€æœ¯ç‰¹æ€§ Technical Features
- âš¡ **é«˜æ€§èƒ½å¼‚æ­¥** - åŸºäºFastAPIå’Œasyncioçš„å¼‚æ­¥æ¶æ„
- ğŸ”’ **å®‰å…¨è®¤è¯** - JWT tokenæ”¯æŒå’ŒAPIå¯†é’¥åŠ å¯†å­˜å‚¨
- ğŸ“Š **ç›‘æ§ç»Ÿè®¡** - è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡
- ğŸ”„ **æ•…éšœæ¢å¤** - è‡ªåŠ¨é‡è¯•å’Œæ•…éšœè½¬ç§»æœºåˆ¶
- ğŸŒ **è·¨åŸŸæ”¯æŒ** - å®Œæ•´çš„CORSé…ç½®
- ğŸ“ **ä¸­æ–‡æ–‡æ¡£** - å®Œæ•´çš„ä¸­æ–‡APIæ–‡æ¡£å’Œæ³¨é‡Š

### éƒ¨ç½²è¯´æ˜ Deployment
æ”¯æŒDockerå®¹å™¨åŒ–éƒ¨ç½²ï¼Œé…ç½®ç®€å•ï¼Œå¼€ç®±å³ç”¨ã€‚
è¯¦ç»†é…ç½®è¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£å’Œç¯å¢ƒå˜é‡è¯´æ˜ã€‚
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan,
    contact={
        "name": "AIæ¨¡å‹æœåŠ¡å›¢é˜Ÿ",
        "email": "ai-service@example.com"
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT"
    }
)

# è·å–é…ç½®
settings = get_settings()

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    logger.error(f"Global exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
                "type": "internal_error"
            }
        }
    )


# 404å¤„ç†
@app.exception_handler(404)
async def not_found_handler(request, exc):
    """404é”™è¯¯å¤„ç†å™¨"""
    return JSONResponse(
        status_code=404,
        content={
            "error": {
                "message": "è¯·æ±‚çš„èµ„æºä¸å­˜åœ¨",
                "type": "not_found"
            }
        }
    )


# æ ¹è·¯å¾„ - Root Endpoint  
@app.get("/", 
         summary="æœåŠ¡æ ¹ç›®å½•",
         description="è·å–AIæ¨¡å‹æœåŠ¡çš„åŸºæœ¬ä¿¡æ¯å’ŒAPIç«¯ç‚¹æ¦‚è§ˆ",
         response_description="æœåŠ¡åŸºæœ¬ä¿¡æ¯å’Œæ”¯æŒçš„ç«¯ç‚¹åˆ—è¡¨")
async def root() -> Dict[str, Any]:
    """
    AIæ¨¡å‹æœåŠ¡æ ¹ç›®å½• - AI Model Service Root Endpoint
    
    åŠŸèƒ½æè¿°:
    è¿”å›æœåŠ¡çš„åŸºæœ¬ä¿¡æ¯ã€ç‰ˆæœ¬å·å’Œæ‰€æœ‰å¯ç”¨APIç«¯ç‚¹çš„æ¦‚è§ˆã€‚
    è¿™æ˜¯äº†è§£æœåŠ¡åŠŸèƒ½çš„å…¥å£ç«¯ç‚¹ã€‚
    
    å“åº”å†…å®¹:
    - service: æœåŠ¡åç§°
    - version: å½“å‰ç‰ˆæœ¬å·
    - description: æœåŠ¡åŠŸèƒ½æè¿°
    - docs: APIæ–‡æ¡£åœ°å€
    - health: å¥åº·æ£€æŸ¥ç«¯ç‚¹
    - supported_endpoints: æ‰€æœ‰å¯ç”¨APIç«¯ç‚¹åˆ—è¡¨
    
    ä½¿ç”¨åœºæ™¯:
    - APIæ¢ç´¢å’Œå‘ç°
    - æœåŠ¡çŠ¶æ€ç¡®è®¤
    - å‰ç«¯é›†æˆå‚è€ƒ
    - ç³»ç»Ÿé›†æˆæŒ‡å—
    """
    return {
        "service": "AIæ¨¡å‹æœåŠ¡",
        "version": "1.0.0",
        "description": "ç»Ÿä¸€çš„AIæ¨¡å‹è°ƒç”¨å’Œç®¡ç†æœåŠ¡",
        "docs": "/docs",
        "health": "/health",
        "supported_endpoints": {
            "chat": "/api/v1/chat/completions",
            "chat_stream": "/api/v1/chat/completions/stream", 
            "models": "/api/v1/models/",
            "providers": "/api/v1/models/providers",
            "status": "/api/v1/status/health",
            "metrics": "/api/v1/status/metrics"
        }
    }


# ç®€å•å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆDockerä½¿ç”¨ï¼‰- Health Check Endpoint for Docker
@app.get("/health",
         summary="æœåŠ¡å¥åº·æ£€æŸ¥",
         description="æ£€æŸ¥AIæ¨¡å‹æœåŠ¡çš„è¿è¡ŒçŠ¶æ€å’Œå¥åº·åº¦",
         response_description="æœåŠ¡å¥åº·çŠ¶æ€ä¿¡æ¯")
async def health_check() -> Dict[str, Any]:
    """
    æœåŠ¡å¥åº·æ£€æŸ¥ç«¯ç‚¹ - Service Health Check Endpoint
    
    åŠŸèƒ½æè¿°:
    æ£€æŸ¥AIæ¨¡å‹æœåŠ¡çš„åŸºæœ¬è¿è¡ŒçŠ¶æ€ï¼ŒåŒ…æ‹¬æœåŠ¡å¯ç”¨æ€§å’ŒAIæ¨¡å‹è¿æ¥çŠ¶æ€ã€‚
    ä¸»è¦ç”¨äºDockerå®¹å™¨å¥åº·æ£€æŸ¥å’Œè´Ÿè½½å‡è¡¡å™¨æ¢æµ‹ã€‚
    
    å“åº”å†…å®¹ (å¥åº·):
    - status: "healthy" - æœåŠ¡çŠ¶æ€æ­£å¸¸
    - service: "ai-model-service" - æœåŠ¡æ ‡è¯†
    - timestamp: æ£€æŸ¥æ—¶é—´æˆ³
    - mode: "simplified"/"full" - æœåŠ¡è¿è¡Œæ¨¡å¼
    
    å“åº”å†…å®¹ (ä¸å¥åº·):
    - status: "unhealthy" - æœåŠ¡çŠ¶æ€å¼‚å¸¸
    - service: "ai-model-service" - æœåŠ¡æ ‡è¯†
    - error: é”™è¯¯è¯¦ç»†ä¿¡æ¯
    
    ä½¿ç”¨åœºæ™¯:
    - Dockerå®¹å™¨å¥åº·æ£€æŸ¥
    - Kubernetes liveness probe
    - è´Ÿè½½å‡è¡¡å™¨å¥åº·æ¢æµ‹
    - ç›‘æ§ç³»ç»ŸçŠ¶æ€æ”¶é›†
    
    æŠ€æœ¯è¯´æ˜:
    - è½»é‡çº§æ£€æŸ¥ï¼Œå“åº”å¿«é€Ÿ
    - åŒ…å«AIæœåŠ¡åˆå§‹åŒ–çŠ¶æ€
    - æ”¯æŒç®€åŒ–æ¨¡å¼å’Œå®Œæ•´æ¨¡å¼
    """
    try:
        ai_service = await get_ai_service()
        health_data = await ai_service.health_check()
        
        return {
            "status": "healthy",
            "service": "ai-model-service",
            "timestamp": health_data.get("timestamp"),
            "mode": "simplified" if health_data.get("simplified_mode") else "full"
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return {
            "status": "unhealthy",
            "service": "ai-model-service", 
            "error": str(e)
        }


# æœåŠ¡ä¿¡æ¯ - Service Information Endpoint
@app.get("/info",
         summary="è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯",
         description="æŸ¥è¯¢AIæ¨¡å‹æœåŠ¡çš„å®Œæ•´é…ç½®å’ŒçŠ¶æ€ä¿¡æ¯",
         response_description="æœåŠ¡è¯¦ç»†ä¿¡æ¯åŒ…æ‹¬é…ç½®ã€çŠ¶æ€å’ŒåŠŸèƒ½ç‰¹æ€§")
async def service_info() -> Dict[str, Any]:
    """
    è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯ç«¯ç‚¹ - Service Information Endpoint
    
    åŠŸèƒ½æè¿°:
    è¿”å›AIæ¨¡å‹æœåŠ¡çš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœåŠ¡é…ç½®ã€è¿è¡ŒçŠ¶æ€ã€
    åŠŸèƒ½ç‰¹æ€§ã€é…ç½®å‚æ•°ç­‰è¯¦ç»†ä¿¡æ¯ã€‚
    
    å“åº”å†…å®¹:
    - service: æœåŠ¡åŸºæœ¬ä¿¡æ¯
      - name: æœåŠ¡åç§°
      - version: ç‰ˆæœ¬å·
      - port: æœåŠ¡ç«¯å£
      - environment: è¿è¡Œç¯å¢ƒ
      - features: åŠŸèƒ½ç‰¹æ€§åˆ—è¡¨
    - status: æœåŠ¡è¿è¡ŒçŠ¶æ€
    - configuration: é…ç½®ä¿¡æ¯
      - storage_service_url: å­˜å‚¨æœåŠ¡URL
      - redis_enabled: Redisç¼“å­˜çŠ¶æ€
      - health_check_interval: å¥åº·æ£€æŸ¥é—´éš”
      - cache_ttl_*: ç¼“å­˜è¿‡æœŸæ—¶é—´é…ç½®
    
    åŠŸèƒ½ç‰¹æ€§åŒ…å«:
    - å¤šå¹³å°AIæ¨¡å‹æ”¯æŒ
    - æ™ºèƒ½æ¨¡å‹è·¯ç”±
    - è´Ÿè½½å‡è¡¡
    - è´¦å·æ± ç®¡ç†
    - å¥åº·ç›‘æ§
    - ä½¿ç”¨ç»Ÿè®¡
    - æˆæœ¬åˆ†æ
    - æµå¼å“åº”
    
    ä½¿ç”¨åœºæ™¯:
    - ç³»ç»Ÿç®¡ç†å’Œç›‘æ§
    - æ•…éšœæ’æŸ¥å’Œè¯Šæ–­
    - é…ç½®éªŒè¯å’Œç¡®è®¤
    - ç³»ç»Ÿé›†æˆå’Œå¯¹æ¥
    - æ€§èƒ½åˆ†æå’Œä¼˜åŒ–
    """
    try:
        ai_service = await get_ai_service()
        service_status = await ai_service.get_service_status()
        
        return {
            "service": {
                "name": "AIæ¨¡å‹æœåŠ¡",
                "version": "1.0.0",
                "port": settings.service_port,
                "environment": "development",  # å¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–
                "features": [
                    "å¤šå¹³å°AIæ¨¡å‹æ”¯æŒ",
                    "æ™ºèƒ½æ¨¡å‹è·¯ç”±",
                    "è´Ÿè½½å‡è¡¡",
                    "è´¦å·æ± ç®¡ç†",
                    "å¥åº·ç›‘æ§",
                    "ä½¿ç”¨ç»Ÿè®¡",
                    "æˆæœ¬åˆ†æ",
                    "æµå¼å“åº”"
                ]
            },
            "status": service_status,
            "configuration": {
                "storage_service_url": settings.storage_service_url,
                "redis_enabled": bool(settings.redis_url),
                "health_check_interval": settings.health_check_interval,
                "cache_ttl_models": settings.cache_ttl_models,
                "cache_ttl_accounts": settings.cache_ttl_accounts
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting service info: {e}")
        return {
            "service": {
                "name": "AIæ¨¡å‹æœåŠ¡", 
                "version": "1.0.0",
                "status": "error"
            },
            "error": str(e)
        }


# æ³¨å†Œè·¯ç”±
chat_controller = create_chat_controller()
models_controller = create_models_controller()
status_controller = create_status_controller()
models_management_controller = create_models_management_controller()

app.include_router(chat_controller.router)
app.include_router(models_controller.router)
app.include_router(status_controller.router)
app.include_router(models_management_controller.router)


# å¼€å‘æ¨¡å¼ä¸‹çš„è°ƒè¯•ç«¯ç‚¹
if settings.debug:
    @app.get("/debug/config")
    async def debug_config():
        """è°ƒè¯•é…ç½®ä¿¡æ¯ï¼ˆä»…å¼€å‘æ¨¡å¼ï¼‰"""
        return {
            "settings": {
                "service_port": settings.service_port,
                "storage_service_url": settings.storage_service_url,
                "redis_url": settings.redis_url and "å·²é…ç½®" or "æœªé…ç½®",
                "debug": settings.debug,
                "log_level": settings.log_level,
                "health_check_interval": settings.health_check_interval,
                "quota_alert_threshold": settings.quota_alert_threshold,
                "cache_ttl_models": settings.cache_ttl_models,
                "cache_ttl_accounts": settings.cache_ttl_accounts,
                "default_routing_strategy": settings.default_routing_strategy,
                "cache_prefix": settings.cache_prefix
            }
        }


if __name__ == "__main__":
    import uvicorn
    
    # åˆ›å»ºlogsç›®å½•
    import os
    os.makedirs("logs", exist_ok=True)
    
    logger.info(f"Starting AI Model Service on port {settings.service_port}")
    
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=settings.service_port,
        reload=settings.debug,
        log_level=settings.log_level.lower()
    )